import torch
import os
import io
from io import BytesIO
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, BitsAndBytesConfig
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_omni_utils import process_mm_info
import numpy as np
import soundfile as sf
import re
import datetime
import torchaudio
device = "cuda"

def check_flash_attention():
    """Check if flash attention 2 is available"""
    try:
        from flash_attn import flash_attn_func
        return True
    except ImportError:
        return False

FLASH_ATTENTION_AVAILABLE = check_flash_attention()

def init_qwen_paths():
    """åŠ¨æ€åˆå§‹åŒ–æ¨¡å‹è·¯å¾„"""
    qwen_base = Path(folder_paths.models_dir) / "Qwen"
    qwen_model_dir = qwen_base / "Qwen2.5-Omni-7B"
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    qwen_model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œè·¯å¾„åˆ°ç³»ç»Ÿ
    if not hasattr(folder_paths, "add_model_folder_path"):
        # å…¼å®¹æ—§ç‰ˆæœ¬æ‰‹åŠ¨æ³¨å†Œ
        if "Qwen" not in folder_paths.folder_names_and_paths:
            folder_paths.folder_names_and_paths["Qwen"] = ([str(qwen_model_dir)], {'.safetensors', '.bin'})
    else:
        folder_paths.add_model_folder_path("Qwen", str(qwen_model_dir))
    
    return str(qwen_model_dir)


class LoadQwenOmniModel:
    def __init__(self):
        self.model_path = init_qwen_paths()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                
            }
        }

    RETURN_TYPES = ("QWENOMNI", "OMNIPROCESSOR")
    RETURN_NAMES = ("model", "processor")
    FUNCTION = "load_model"
    CATEGORY = "ğŸ¼QwenOmni"
  
    def load_model(self):
        # æ·»åŠ Flash Attentionæ”¯æŒåˆ¤æ–­
        attn_implementation = "flash_attention_2" if FLASH_ATTENTION_AVAILABLE else None
        # æ·»åŠ é‡åŒ–é…ç½®
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,  # æ”¹ç”¨FP16åŠ é€Ÿè®¡ç®—
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,        # æ‰“å¼€orå…³é—­åŒé‡é‡åŒ–
            llm_int8_threshold=6.0,
        )

        


        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨æ›´é«˜æ•ˆçš„BF16æ ¼å¼
            quantization_config=quant_config,
            attn_implementation=attn_implementation,
            low_cpu_mem_usage=True,
            use_safetensors=True,  # å¯ç”¨æ›´å¿«çš„safetensorsæ ¼å¼
            offload_state_dict=True,  # ä¼˜åŒ–æ˜¾å­˜åˆ†é…
            enable_audio_output=True
        ).eval()

        
        # é¢„åŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜
        
        processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
        return model, processor

class QwenOmniParser:

    def __init__(self):
        self.model = None
        self.processor = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("QWENOMNI",),
                "processor": ("OMNIPROCESSOR",),
                "image": ("IMAGE",),
                "prompt": ("STRING", {
                    "default": "Describe this image in detail", 
                    "multiline": True
                }),
                "max_tokens": ("INT", {
                    "default": 128, 
                    "min": 32, 
                    "max": 512
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1
                }),
                "audio_mode": ([
                    "ğŸ”‡None (No Audio)", 
                    "ğŸ‘±â€â™€ï¸Chelsie (Female)", 
                    "ğŸ‘¨ğŸ»Ethan (Male)"
                ], {"default": "ğŸ”‡None (No Audio)"}),
            }
        }

    RETURN_TYPES = ("STRING", "AUDIO")
    RETURN_NAMES = ("text", "audio")
    FUNCTION = "analyze_image"
    CATEGORY = "ğŸ¼QwenOmni"

    def tensor_to_pil(self, image_tensor):
        """ä¼˜åŒ–å›¾åƒå¼ é‡è½¬æ¢"""
        # å¤„ç†æ‰¹æ¬¡ç»´åº¦ [B x H x W x C]
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
            
        # æ•°å€¼èŒƒå›´è½¬æ¢ [0-1] => [0-255]
        image_np = image_tensor.cpu().numpy()
        if image_np.max() <= 1.0:
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image_np.astype(np.uint8)
            
        return Image.fromarray(image_np)

    @torch.no_grad()
    def analyze_image(self, model, processor, image, prompt, max_tokens, temperature, audio_mode):
        # è½¬æ¢è¾“å…¥æ ¼å¼
        pil_image = self.tensor_to_pil(image)
        
        # å®šä¹‰ç³»ç»Ÿæç¤ºå¸¸é‡
        DEFAULT_SYSTEM_PROMPT = "AI Assistant"
        OFFICIAL_AUDIO_PROMPT = "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."

        # è§£æéŸ³é¢‘å‚æ•°
        enable_audio = audio_mode != "ğŸ”‡None (No Audio)"
        voice_type = "Chelsie" if "Chelsie" in audio_mode else "Ethan" if "Ethan" in audio_mode else None

        # é˜¶æ®µä¸€ï¼šç”Ÿæˆæ ¸å¿ƒæ–‡æœ¬
        def generate_core_text():
            conversation = [
                {"role": "system", "content": [{"type": "text", "text": DEFAULT_SYSTEM_PROMPT}]},
                {"role": "user", "content": [
                    {"type": "image", "image": pil_image},
                    {"type": "text", "text": prompt}
                ]}
            ]
            inputs = processor(
                text=processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False), # å…³é—­è‡ªåŠ¨æ·»åŠ è§’è‰²æç¤º
                images=[pil_image],
                return_tensors="pt",
                padding=True
            ).to(model.device)
            
            generate_config = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "temperature": temperature,
                "use_cache": True,
                "return_audio": False
            }
            text_ids = model.generate(**inputs, **generate_config)
            return processor.batch_decode(text_ids, skip_special_tokens=True)[0]

        # é˜¶æ®µäºŒï¼šåŸºäºæ–‡æœ¬ç”Ÿæˆè¯­éŸ³
        def generate_speech(text):
            conversation = [
                {"role": "system", "content": [{"type": "text", "text": OFFICIAL_AUDIO_PROMPT}]},
                {"role": "user", "content": [
                    {"type": "text", "text": f"<|im_start|>user\n{text}<|im_end|>"}  # âœ… ä½¿ç”¨åŸå§‹æ–‡æœ¬æ ‡è®°
                ]}
            ]
            inputs = processor(
                text=processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False), # å…³é—­è‡ªåŠ¨æ·»åŠ è§’è‰²æç¤º
                return_tensors="pt",
                padding=True
            ).to(model.device)
            
            # æ™ºèƒ½é…ç½®ç”Ÿæˆå‚æ•° â–¼â–¼â–¼
            generate_config = {
                "max_new_tokens": len(text.split()) * 3,
                "do_sample": False,
                "use_cache": True,
                "return_audio": True
            }
            
            # æœ‰æ•ˆæ€§éªŒè¯åæ·»åŠ å‘éŸ³äººå‚æ•° â–¼â–¼â–¼
            if voice_type in {"Chelsie", "Ethan"}:  # ä½¿ç”¨é›†åˆåŠ é€Ÿåˆ¤æ–­
                generate_config["speaker"] = voice_type
            else:
                print(f"[WARN] ä½¿ç”¨æ¨¡å‹é»˜è®¤å‘éŸ³äººï¼Œå½“å‰é€‰æ‹©: {audio_mode}")
            
            _, audio = model.generate(**inputs, **generate_config)
            return audio

        # ä¸»æµç¨‹
        text = generate_core_text()
        audio = torch.zeros(0)
        
        if enable_audio:
            # äºŒæ¬¡éªŒè¯å‘éŸ³äººæœ‰æ•ˆæ€§ â–¼â–¼â–¼
            if voice_type is None:
                raise ValueError(f"æ— æ•ˆçš„å‘éŸ³äººé…ç½®ï¼Œaudio_mode: {audio_mode}")
            audio = generate_speech(text)

        return (text, audio)
    


class SaveQwenOmniAudio:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "filename": ("STRING", {"default": "output.wav"}),
                "samplerate": ("INT", {"default": 23000}),
            }
        }
    
    RETURN_TYPES = ()
    FUNCTION = "save_audio"
    OUTPUT_NODE = True
    CATEGORY = "ğŸ¼QwenOmni"    
    def save_audio(self, audio, filename, samplerate):
        # è·å–ComfyUIçš„è¾“å‡ºç›®å½•
        output_dir = folder_paths.get_output_directory()
        # ç”Ÿæˆæ—¥æœŸéƒ¨åˆ†ï¼ˆyyyyMMddï¼‰
        date_str = datetime.datetime.now().strftime("%Y%m%d")
        # æŸ¥æ‰¾å½“å¤©æœ€æ–°åºå·
        existing_files = os.listdir(output_dir)
        pattern = re.compile(rf"^{date_str}_(\d{{4}})\.wav$")
        # æå–å·²æœ‰åºå·å¹¶æ‰¾åˆ°æœ€å¤§å€¼
        max_sequence = 0
        for filename in existing_files:
            match = pattern.match(filename)
            if match:
                current_seq = int(match.group(1))
                max_sequence = max(max_sequence, current_seq)
        
        # ç”Ÿæˆæ–°åºå·ï¼ˆè‡ªåŠ¨é€’å¢ï¼‰
        new_sequence = max_sequence + 1
        
        # ç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„ï¼ˆé˜²æ­¢ç›®å½•æ³¨å…¥ï¼‰
        filename = os.path.basename(filename)
        # æ„å»ºå®Œæ•´æ–‡ä»¶å
        new_filename = f"{date_str}_{new_sequence:04d}.wav"
        full_path = os.path.join(output_dir, new_filename)
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        
        # # æ„å»ºå®Œæ•´ä¿å­˜è·¯å¾„
        # full_path = os.path.join(output_dir, filename)
        
        # # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        # os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        sf.write(
            full_path,
            audio.reshape(-1).detach().cpu().numpy(),
            samplerate=samplerate,
        )
        # return ()
        return {"ui": {"audio": [full_path]}}



NODE_CLASS_MAPPINGS = {
    "LoadQwenOmniModel": LoadQwenOmniModel,
    "QwenOmniParser": QwenOmniParser,
    "SaveQwenOmniAudio": SaveQwenOmniAudio,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LoadQwenOmniModel": "Load QwenOmni ModelğŸ¼",
    "QwenOmniParser": "QwenOmni ParserğŸ¼",
    "SaveQwenOmniAudio": "Save QwenOmni AudioğŸ¼",
}