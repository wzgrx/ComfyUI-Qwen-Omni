from __future__ import annotations
import torch
import os
import tempfile
import io
import torchaudio
from transformers import Qwen2_5OmniForConditionalGeneration, AutoProcessor, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as modelscope_snapshot_download
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_omni_utils import process_mm_info
import numpy as np
import soundfile as sf
import requests
import time
from .VideoUploader import VideoUploader


def check_flash_attention():
    """æ£€æµ‹Flash Attention 2æ”¯æŒï¼ˆéœ€Ampereæ¶æ„åŠä»¥ä¸Šï¼‰"""
    try:
        from flash_attn import flash_attn_func
        major, _ = torch.cuda.get_device_capability()
        return major >= 8  # ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›8.0+çš„GPUï¼ˆå¦‚RTX 30ç³»åŠä»¥ä¸Šï¼‰
    except ImportError:
        return False


FLASH_ATTENTION_AVAILABLE = check_flash_attention()


def init_qwen_paths():
    """åˆå§‹åŒ–æ¨¡å‹è·¯å¾„ï¼Œç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„"""
    base_dir = Path(folder_paths.models_dir).resolve()
    qwen_dir = base_dir / "Qwen" # æ·»åŠ VLMå­ç›®å½•å¦‚ / "Qwen" / "VLM"
    model_dir = qwen_dir / "Qwen2.5-Omni-7B"    

    
    # åˆ›å»ºç›®å½•
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œåˆ°ComfyUI
    if hasattr(folder_paths, "add_model_folder_path"):
        folder_paths.add_model_folder_path("Qwen", str(model_dir))
    else:
        folder_paths.folder_names_and_paths["Qwen"] = ([str(model_dir)], {'.safetensors', '.bin'})
    
    print(f"æ¨¡å‹è·¯å¾„å·²åˆå§‹åŒ–: {model_dir}")
    return str(model_dir)


def test_download_speed(url):
    """æµ‹è¯•ä¸‹è½½é€Ÿåº¦ï¼Œä¸‹è½½ 5 ç§’"""
    try:
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=10)
        downloaded_size = 0
        for data in response.iter_content(chunk_size=1024):
            if time.time() - start_time > 5:
                break
            downloaded_size += len(data)
        end_time = time.time()
        speed = downloaded_size / (end_time - start_time) / 1024  # KB/s
        return speed
    except Exception as e:
        print(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦æ—¶å‡ºç°é”™è¯¯: {e}")
        return 0


def validate_model_path(model_path):
    """éªŒè¯æ¨¡å‹è·¯å¾„çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨"""
    path_obj = Path(model_path)
    
    # åŸºæœ¬è·¯å¾„æ£€æŸ¥
    if not path_obj.is_absolute():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç»å¯¹è·¯å¾„")
        return False
    
    if not path_obj.exists():
        print(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not path_obj.is_dir():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç›®å½•")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    if not check_model_files_exist(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {model_path}")
        return False
    
    return True




def check_model_files_exist(model_dir):
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨"""
    required_files = [
        "added_tokens.json",
        "chat_template.json",
        "merges.txt",
        "model.safetensors.index.json",
        "preprocessor_config.json",
        "spk_dict.pt",
        "tokenizer.json",
        "vocab.json",
        "config.json",
        "generation_config.json",
        "model-00001-of-00005.safetensors",
        "model-00002-of-00005.safetensors",
        "model-00003-of-00005.safetensors",
        "model-00004-of-00005.safetensors",
        "model-00005-of-00005.safetensors",
        "special_tokens_map.json",
        "tokenizer_config.json"
    ]
    for file in required_files:
        if not os.path.exists(os.path.join(model_dir, file)):
            return False
    return True



class QwenOmniCombined:
    def __init__(self):
        # é‡ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹²æ‰°
        os.environ.pop("HUGGINGFACE_HUB_CACHE", None)     

        self.model_path = init_qwen_paths()
        self.cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"ç¼“å­˜è·¯å¾„: {self.cache_dir}")
        
        # éªŒè¯å¹¶åˆ›å»ºç¼“å­˜ç›®å½•
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        self.model = None
        self.processor = None
        self.tokenizer = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                    ["Qwen2.5-Omni-7B"],
                    {
                        "default": "Qwen2.5-Omni-7B",
                        "tooltip": "Select the available model version. Currently, only the Qwen2.5-Omni-7B multimodal large model is supported."
                    }
                ),
                "quantization": (
                    [
                        "ğŸ‘ 4-bit (VRAM-friendly)",
                        "âš–ï¸ 8-bit (Balanced Precision)",
                        "ğŸš« None (Original Precision)"
                    ],
                    {
                        "default": "ğŸ‘ 4-bit (VRAM-friendly)",
                        "tooltip": "Select the quantization level:\nâœ… 4-bit: Significantly reduces VRAM usage, suitable for resource-constrained environments.\nâš–ï¸ 8-bit: Strikes a balance between precision and performance.\nğŸš« None: Uses the original floating-point precision (requires a high-end GPU)."
                    }
                ),
                "prompt": (
                    "STRING",
                    {
                        "default": "Hi!ğŸ˜½",
                        "multiline": True,
                        "tooltip": "Enter a text prompt, supporting Chinese and emojis. Example: 'Describe a cat in a painter's style.'"
                    }
                ),
                "audio_output": (
                    [
                        "ğŸ”‡None (No Audio)",
                        "ğŸ‘±â€â™€ï¸Chelsie (Female)",
                        "ğŸ‘¨â€ğŸ¦°Ethan (Male)"
                    ],
                    {
                        "default": "ğŸ”‡None (No Audio)",
                        "tooltip": "Audio output options:\nğŸ”‡ Do not generate audio.\nğŸ‘±â€â™€ï¸ Use the female voice Chelsie (warm tone).\nğŸ‘¨â€ğŸ¦° Use the male voice Ethan (calm tone)."
                    }
                ),
                "audio_source": (
                    [
                        "ğŸ§ Separate Audio Input",
                        "ğŸ¬ Video Built-in Audio"
                    ],
                    {
                        "default": "ğŸ§ Separate Audio Input",
                        "display": "radio",
                        "tooltip": "Select audio source: Use video's built-in audio track (priority) / Input a separate audio file (external audio)"
                    }
                ),
                "max_tokens": (
                    "INT",
                    {
                        "default": 132,
                        "min": 64,
                        "max": 2048,
                        "step": 16,
                        "display": "slider",
                        "tooltip": "Control the maximum length of the generated text (in tokens). \nGenerally, 100 tokens correspond to approximately 50 - 100 Chinese characters or 67 - 100 English words, but the actual number may vary depending on the text content and the model's tokenization strategy. \nRecommended range: 64 - 512."
                    }
                ),
                "temperature": (
                    "FLOAT",
                    {
                        "default": 0.4,
                        "min": 0.1,
                        "max": 1.0,
                        "step": 0.1,
                        "display": "slider",
                        "tooltip": "Control the generation diversity:\nâ–«ï¸ 0.1 - 0.3: Generate structured/technical content.\nâ–«ï¸ 0.5 - 0.7: Balance creativity and logic.\nâ–«ï¸ 0.8 - 1.0: High degree of freedom (may produce incoherent content)."
                    }
                ),
                "top_p": (
                    "FLOAT",
                    {
                        "default": 0.9,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "display": "slider",
                        "tooltip": "Nucleus sampling threshold:\nâ–ªï¸ Close to 1.0: Retain more candidate words (more random).\nâ–ªï¸ 0.5 - 0.8: Balance quality and diversity.\nâ–ªï¸ Below 0.3: Generate more conservative content."
                    }
                ),
                "repetition_penalty": (
                    "FLOAT",
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 2.0,
                        "step": 0.01,
                        "display": "slider",
                        "tooltip": "Control of repeated content:\nâš ï¸ 1.0: Default behavior.\nâš ï¸ >1.0 (Recommended 1.2): Suppress repeated phrases.\nâš ï¸ <1.0 (Recommended 0.8): Encourage repeated emphasis."
                    }
                )
            },
            "optional": {
                "image": (
                    "IMAGE",
                    {
                        "tooltip": "Upload a reference image (supports PNG/JPG), and the model will adjust the generation result based on the image content."
                    }
                ),
                "audio": (
                    "AUDIO",
                    {
                        "tooltip": "Upload an audio file (supports MP3/WAV), and the model will analyze the audio content and generate relevant responses."
                    }
                ),
                "video_path": (
                    "VIDEO_PATH",
                    {
                        "tooltip": "Enter the video file  (supports MP4/WEBM), and the model will extract visual features to assist in generation."
                    }
                )
            }
        }

    RETURN_TYPES = ("STRING", "AUDIO")
    RETURN_NAMES = ("text", "audio")
    FUNCTION = "process"
    CATEGORY = "ğŸ¼QwenOmni"    

    def load_model(self, model_name, quantization):
        # æ·»åŠ CUDAå¯ç”¨æ€§æ£€æŸ¥
        if not torch.cuda.is_available():
            raise RuntimeError(f"CUDA is required for  {model_name} model")

        quant_config = None
        if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
        elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True
            )

        # è‡ªå®šä¹‰device_mapï¼Œè¿™é‡Œå‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ¨¡å‹å°½å¯èƒ½æ”¾åˆ°GPUä¸Š
        device_map = {"": 0} if torch.cuda.device_count() > 0 else "auto"



        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
        if not validate_model_path(self.model_path):
            print(f"æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸ºä½ ä¸‹è½½ {model_name} æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            print(f"ä¸‹è½½å°†ä¿å­˜åœ¨: {self.model_path}")
            
            # å¼€å§‹ä¸‹è½½é€»è¾‘
            try:
                # æµ‹è¯•ä¸‹è½½é€Ÿåº¦
                huggingface_test_url = "https://huggingface.co/Qwen/Qwen2.5-Omni-7B/resolve/main/model-00005-of-00005.safetensors"
                modelscope_test_url = "https://modelscope.cn/api/v1/models/qwen/Qwen2.5-Omni-7B/repo?Revision=master&FilePath=model-00005-of-00005.safetensors"
                huggingface_speed = test_download_speed(huggingface_test_url)
                modelscope_speed = test_download_speed(modelscope_test_url)


                print(f"Hugging Faceä¸‹è½½é€Ÿåº¦: {huggingface_speed:.2f} KB/s")
                print(f"ModelScopeä¸‹è½½é€Ÿåº¦: {modelscope_speed:.2f} KB/s")

                # ä¼˜åŒ–åˆ¤æ–­æ¡ä»¶ï¼šåªæœ‰å½“Hugging Faceé€Ÿåº¦è¶…è¿‡ModelScope 50%æ—¶æ‰ä¼˜å…ˆé€‰æ‹©

                if huggingface_speed > modelscope_speed * 1.5:
                    download_sources = [
                        (snapshot_download, "Qwen/Qwen2.5-Omni-7B", "Hugging Face"),
                        (modelscope_snapshot_download, "qwen/Qwen2.5-Omni-7B", "ModelScope")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»Hugging Faceä¸‹è½½")
                else:
                    download_sources = [
                        (modelscope_snapshot_download, "qwen/Qwen2.5-Omni-7B", "ModelScope"),
                        (snapshot_download, "Qwen/Qwen2.5-Omni-7B", "Hugging Face")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»ModelScopeä¸‹è½½")

                max_retries = 3
                success = False
                final_error = None
                used_cache_path = None

                for download_func, repo_id, source in download_sources:
                    for retry in range(max_retries):
                        try:
                            print(f"å¼€å§‹ä» {source} ä¸‹è½½æ¨¡å‹ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰...")
                            if download_func == snapshot_download:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    ignore_patterns=["*.msgpack", "*.h5"]
                                )
                            else:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir
                                )

                            used_cache_path = cached_path  # è®°å½•ä½¿ç”¨çš„ç¼“å­˜è·¯å¾„
                            
                            # å°†ä¸‹è½½çš„æ¨¡å‹å¤åˆ¶åˆ°æ¨¡å‹ç›®å½•
                            self.copy_cached_model_to_local(cached_path, self.model_path)
                            
                            print(f"æˆåŠŸä» {source} ä¸‹è½½æ¨¡å‹åˆ° {self.model_path}")

                            # ä¸‹è½½æˆåŠŸæç¤º
                            print("\nâš ï¸ æ³¨æ„ï¼šæ¨¡å‹ä¸‹è½½è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç¼“å­˜æ–‡ä»¶")
                            print(f"ç¼“å­˜è·¯å¾„: {cached_path}")
                            print("ä¸ºé¿å…å ç”¨é¢å¤–ç¡¬ç›˜ç©ºé—´ï¼Œä½ å¯ä»¥åœ¨ç¡®è®¤æ¨¡å‹æ­£å¸¸å·¥ä½œååˆ é™¤æ­¤ç¼“å­˜ç›®å½•")
                            
                            success = True
                            break

                        except Exception as e:
                            final_error = e  # ä¿å­˜æœ€åä¸€ä¸ªé”™è¯¯
                            if retry < max_retries - 1:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå³å°†è¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•...")
                            else:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå°è¯•å…¶ä»–æº...")
                    if success:
                        break
                else:
                    raise RuntimeError("ä»æ‰€æœ‰æºä¸‹è½½æ¨¡å‹å‡å¤±è´¥ã€‚")
                
                # ä¸‹è½½å®Œæˆåå†æ¬¡éªŒè¯
                if not validate_model_path(self.model_path):
                    raise RuntimeError(f"ä¸‹è½½åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {self.model_path}")
                
                print(f"æ¨¡å‹ {model_name} å·²å‡†å¤‡å°±ç»ª")
                
            except Exception as e:
                print(f"ä¸‹è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                
                # ä¸‹è½½å¤±è´¥æç¤º
                if used_cache_path:
                    print("\nâš ï¸ æ³¨æ„ï¼šä¸‹è½½è¿‡ç¨‹ä¸­åˆ›å»ºäº†ç¼“å­˜æ–‡ä»¶")
                    print(f"ç¼“å­˜è·¯å¾„: {used_cache_path}")
                    print("ä½ å¯ä»¥å‰å¾€æ­¤è·¯å¾„åˆ é™¤ç¼“å­˜æ–‡ä»¶ä»¥é‡Šæ”¾ç¡¬ç›˜ç©ºé—´")
                
                raise RuntimeError(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_name}ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ° {self.model_path}")

        # æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œæ­£å¸¸åŠ è½½
        print(f"åŠ è½½æ¨¡å‹: {self.model_path}")
        self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_path,
            device_map=device_map,
            torch_dtype=torch.float16,
            quantization_config=quant_config,
            attn_implementation="flash_attention_2" if FLASH_ATTENTION_AVAILABLE else "sdpa",
            low_cpu_mem_usage=True,
            use_safetensors=True,
            offload_state_dict=True,
            enable_audio_output=True,
        ).eval()

        # âœ… ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.2+ï¼‰
        if torch.__version__ >= "2.2":
            self.model = torch.compile(self.model, mode="reduce-overhead")

        # âœ… SDPä¼˜åŒ–ï¼ˆæ¨èï¼‰
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)

        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)

    def tensor_to_pil(self, image_tensor):
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
        image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        return Image.fromarray(image_np)

    @torch.no_grad()
    def process(self, model_name, quantization, prompt, audio_output, audio_source, max_tokens, temperature, top_p,
                repetition_penalty, audio=None, image=None, video_path=None):
        if self.model is None or self.processor is None:
            self.load_model(model_name, quantization)

        pil_image = None
        if image is not None:
            pil_image = self.tensor_to_pil(image)
            max_res = 1024
            if max(pil_image.size) > max_res:
                pil_image.thumbnail((max_res, max_res))
                pil_image = np.array(pil_image)
                pil_image = torch.from_numpy(pil_image).permute(2, 0, 1).unsqueeze(0) / 255.0
                pil_image = Image.fromarray((pil_image.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))

        audio_path = None
        temp_audio_file = None

        if audio:
            try:
                temp_audio_file = tempfile.NamedTemporaryFile(suffix=".flac", delete=False)
                audio_path = temp_audio_file.name
                waveform = audio["waveform"].squeeze(0).cpu().numpy()
                sample_rate = audio["sample_rate"]
                sf.write(audio_path, waveform.T, sample_rate)
            except Exception as e:
                print(f"Error saving audio to temporary file: {e}")
                audio_path = None

        SYSTEM_PROMPT = "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."

        conversation = [
            {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
            {"role": "user", "content": []}
        ]

        if pil_image is not None:
            conversation[-1]["content"].append({"type": "image", "image": pil_image})

        # æ·»åŠ éŸ³é¢‘/è§†é¢‘è¾“å…¥ï¼ˆç›´æ¥ä¼ é€’è·¯å¾„ï¼Œç”± qwen-omni-utils å¤„ç†ï¼‰
        use_video_audio = audio_source == "ğŸ¬ Video Built-in Audio"
        if audio_path and not use_video_audio:
            conversation[-1]["content"].append({"type": "audio", "audio": audio_path})
        if video_path:
            conversation[-1]["content"].append({"type": "video", "video": video_path})  # ç›´æ¥æ·»åŠ è§†é¢‘è·¯å¾„

        user_prompt = prompt if prompt.endswith(("?", ".", "ï¼", "ã€‚", "ï¼Ÿ", "ï¼")) else f"{prompt} "
        conversation[-1]["content"].append({"type": "text", "text": user_prompt})

        input_text = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)

        processor_args = {
            "text": input_text,
            "return_tensors": "pt",
            "padding": True,
            "use_audio_in_video": use_video_audio
        }

        # ç›´æ¥è°ƒç”¨ qwen-omni-utils çš„å¤šæ¨¡æ€å¤„ç†é€»è¾‘
        audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_video_audio)
        processor_args["audio"] = audios
        processor_args["images"] = images
        processor_args["videos"] = videos

        inputs = self.processor(**processor_args).to(self.model.device)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        model_inputs = {
            k: v.to(self.device)
            for k, v in inputs.items()
            if v is not None
        }

        generate_config = {
            "max_new_tokens": max(max_tokens, 10),
            "temperature": temperature,
            "do_sample": True,
            "use_cache": True,
            "return_audio": audio_output != "ğŸ”‡None (No Audio)",
            "use_audio_in_video": use_video_audio,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
        }

        if generate_config["return_audio"]:
            generate_config["speaker"] = "Chelsie" if "Chelsie" in audio_output else "Ethan"

        outputs = self.model.generate(**model_inputs, **generate_config)

        # ç»Ÿä¸€æ‰¹æ¬¡ç»´åº¦ï¼Œç¡®ä¿æ–‡æœ¬tokenæ˜¯äºŒç»´å¼ é‡
        if generate_config["return_audio"]:
            text_tokens = outputs[0] if outputs[0].dim() == 2 else outputs[0].unsqueeze(0)
            audio_tensor = outputs[1]
        else:
            text_tokens = outputs if outputs.dim() == 2 else outputs.unsqueeze(0)
            audio_tensor = torch.zeros(0, 0, device=self.model.device)

        # å…³é”®ä¿®æ­£ï¼šå¯¹ text_tokens è¿›è¡Œ token åˆ‡ç‰‡å¤„ç†
        input_length = model_inputs["input_ids"].shape[1]
        text_tokens = text_tokens[:, input_length:]  # æˆªå–æ–°ç”Ÿæˆçš„token

        # ç›´æ¥è·å–å®Œæ•´çš„ç”Ÿæˆæ–‡æœ¬
        text = self.tokenizer.decode(
            text_tokens[0],  # ä½¿ç”¨æ­£ç¡®çš„å˜é‡
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if temp_audio_file:
            try:
                os.remove(temp_audio_file.name)
            except Exception as e:
                print(f"Error deleting temporary audio file: {e}")
        if use_video_audio and 'video_audio_path' in locals():
            try:
                os.remove(video_audio_path)
            except Exception as e:
                print(f"Error deleting video audio temp file: {e}")

        # å¤„ç†éŸ³é¢‘éƒ¨åˆ†ï¼ˆä¸å˜ï¼‰
        if generate_config["return_audio"]:
            audio = audio_tensor
            if isinstance(audio, np.ndarray):
                audio = torch.from_numpy(audio).to(self.model.device)
            if audio.dim() == 1:
                audio = audio.unsqueeze(0)
        else:
            audio = torch.zeros(0, 0, device=self.model.device)

        if audio.dim() == 3:
            audio = audio.mean(dim=1)
        assert audio.dim() == 2, f"Audio waveform must be 2D, got {audio.dim()}D"

        audio_output_data = {
            "waveform": audio,
            "sample_rate": 24000
        }

        if generate_config["return_audio"]:
            buffer = io.BytesIO()
            torchaudio.save(buffer, audio_output_data["waveform"].cpu(), 24000, format="wav")
            buffer.seek(0)
            waveform, sample_rate = torchaudio.load(buffer)
            audio_output_data = {
                "waveform": waveform.unsqueeze(0),
                "sample_rate": sample_rate
            }

        del outputs
        torch.cuda.empty_cache()

        return (text.strip(), audio_output_data)





NODE_CLASS_MAPPINGS = {
    "VideoUploader": VideoUploader,
    "QwenOmniCombined": QwenOmniCombined
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoUploader": "Video UploaderğŸ¼",
    "QwenOmniCombined": "Qwen Omni CombinedğŸ¼"
}
    