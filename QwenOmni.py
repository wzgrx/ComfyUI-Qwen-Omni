from __future__ import annotations
import torch
import os
import tempfile
import io
import torchaudio
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as modelscope_snapshot_download
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_omni_utils import process_mm_info
import numpy as np
import soundfile as sf
import requests
import time
from .VideoUploader import VideoUploader
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_flash_attention():
    """æ£€æµ‹Flash Attention 2æ”¯æŒï¼ˆéœ€Ampereæ¶æ„åŠä»¥ä¸Šï¼‰"""
    try:
        from flash_attn import flash_attn_func
        major, _ = torch.cuda.get_device_capability()
        return major >= 8  # ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›8.0+çš„GPUï¼ˆå¦‚RTX 30ç³»åŠä»¥ä¸Šï¼‰
    except ImportError:
        return False

FLASH_ATTENTION_AVAILABLE = check_flash_attention()

class QwenModelLoader:
    """å¤„ç†Qwenæ¨¡å‹åŠ è½½çš„å…¼å®¹æ€§é—®é¢˜"""
    @staticmethod
    def get_model_class():
        try:
            from transformers import Qwen2_5OmniForConditionalGeneration
            logger.info("ä½¿ç”¨åŸç”Ÿ Qwen2_5OmniForConditionalGeneration æ¨¡å‹ç±»")
            return Qwen2_5OmniForConditionalGeneration
        except ImportError:
            logger.warning("Qwen2_5OmniForConditionalGeneration ä¸å¯ç”¨ï¼Œä½¿ç”¨ AutoModelForCausalLM æ›¿ä»£")
            return AutoModelForCausalLM

def init_qwen_paths():
    """åŠ¨æ€æ³¨å†Œæ¨¡å‹è·¯å¾„ï¼ˆæ”¯æŒComfyUIæ¨¡å‹ç®¡ç†ï¼‰"""
    qwen_dir = Path(folder_paths.models_dir) / "Qwen"
    model_dir = qwen_dir / "Qwen2.5-Omni-7B"
    model_dir.mkdir(parents=True, exist_ok=True)

    # å…¼å®¹ComfyUIæ–°æ—§ç‰ˆæœ¬è·¯å¾„æ³¨å†Œ
    if hasattr(folder_paths, "add_model_folder_path"):
        folder_paths.add_model_folder_path("Qwen", str(model_dir))
    else:
        folder_paths.folder_names_and_paths["Qwen"] = ([str(model_dir)], {'.safetensors', '.bin'})

    return str(model_dir)

def test_download_speed(url):
    """æµ‹è¯•ä¸‹è½½é€Ÿåº¦ï¼Œä¸‹è½½ 5 ç§’"""
    try:
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=10)
        downloaded_size = 0
        for data in response.iter_content(chunk_size=1024):
            if time.time() - start_time > 5:
                break
            downloaded_size += len(data)
        end_time = time.time()
        speed = downloaded_size / (end_time - start_time) / 1024  # KB/s
        return speed
    except Exception as e:
        logger.error(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦æ—¶å‡ºç°é”™è¯¯: {e}")
        return 0

def check_model_files_exist(model_dir):
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨"""
    required_files = [
        "added_tokens.json",
        "chat_template.json",
        "merges.txt",
        "model.safetensors.index.json",
        "preprocessor_config.json",
        "spk_dict.pt",
        "tokenizer.json",
        "vocab.json",
        "config.json",
        "generation_config.json",
        "model-00001-of-00005.safetensors",
        "model-00002-of-00005.safetensors",
        "model-00003-of-00005.safetensors",
        "model-00004-of-00005.safetensors",
        "model-00005-of-00005.safetensors",
        "special_tokens_map.json",
        "tokenizer_config.json"
    ]
    for file in required_files:
        if not os.path.exists(os.path.join(model_dir, file)):
            return False
    return True

class QwenOmniCombined:
    def __init__(self):
        self.model_path = init_qwen_paths()
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.model_class = QwenModelLoader.get_model_class()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                    ["Qwen2.5-Omni-7B"],
                    {"default": "Qwen2.5-Omni-7B"}
                ),
                "quantization": (
                    ["ğŸ‘ 4-bit (VRAM-friendly)", "âš–ï¸ 8-bit (Balanced Precision)", "ğŸš« None (Original Precision)"],
                    {"default": "ğŸ‘ 4-bit (VRAM-friendly)"}
                ),
                "prompt": ("STRING", {"default": "Hi!ğŸ˜½", "multiline": True}),
                "audio_output": (
                    ["ğŸ”‡None (No Audio)", "ğŸ‘±â€â™€ï¸Chelsie (Female)", "ğŸ‘¨â€ğŸ¦°Ethan (Male)"],
                    {"default": "ğŸ”‡None (No Audio)"}
                ),
                "audio_source": (
                    ["ğŸ§ Separate Audio Input", "ğŸ¬ Video Built-in Audio"],
                    {"default": "ğŸ§ Separate Audio Input", "display": "radio"}
                ),
                "max_tokens": ("INT", {"default": 132, "min": 64, "max": 2048, "step": 16, "display": "slider"}),
                "temperature": ("FLOAT", {"default": 0.4, "min": 0.1, "max": 1.0, "step": 0.1, "display": "slider"}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01, "display": "slider"}),
                "repetition_penalty": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01, "display": "slider"})
            },
            "optional": {
                "image": ("IMAGE", {}),
                "audio": ("AUDIO", {}),
                "video_path": ("VIDEO_PATH", {})
            }
        }

    RETURN_TYPES = ("STRING", "AUDIO")
    RETURN_NAMES = ("text", "audio")
    FUNCTION = "process"
    CATEGORY = "ğŸ¼QwenOmni"

    def load_model(self, model_name, quantization):
        if not torch.cuda.is_available():
            raise RuntimeError(f"CUDA is required for {model_name} model")

        quant_config = None
        if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
        elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
            quant_config = BitsAndBytesConfig(load_in_8bit=True)

        device_map = {"": 0} if torch.cuda.device_count() > 0 else "auto"

        if not check_model_files_exist(self.model_path):
            self.download_model()

        logger.info(f"ä½¿ç”¨æ¨¡å‹ç±»: {self.model_class.__name__}")
        self.model = self.model_class.from_pretrained(
            self.model_path,
            device_map=device_map,
            torch_dtype=torch.float16,
            quantization_config=quant_config,
            attn_implementation="flash_attention_2" if FLASH_ATTENTION_AVAILABLE else "sdpa",
            trust_remote_code=True
        ).eval()

        if torch.__version__ >= "2.2":
            self.model = torch.compile(self.model, mode="reduce-overhead")

        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)

        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)

    def download_model(self):
        """å¤„ç†æ¨¡å‹ä¸‹è½½é€»è¾‘"""
        huggingface_test_url = "https://huggingface.co/Qwen/Qwen2.5-Omni-7B/resolve/main/model-00005-of-00005.safetensors"
        modelscope_test_url = "https://modelscope.cn/api/v1/models/qwen/Qwen2.5-Omni-7B/repo?Revision=master&FilePath=model-00005-of-00005.safetensors"
        
        huggingface_speed = test_download_speed(huggingface_test_url)
        modelscope_speed = test_download_speed(modelscope_test_url)

        download_sources = [
            (snapshot_download, "Qwen/Qwen2.5-Omni-7B", "Hugging Face"),
            (modelscope_snapshot_download, "qwen/Qwen2.5-Omni-7B", "ModelScope")
        ] if huggingface_speed >= modelscope_speed else [
            (modelscope_snapshot_download, "qwen/Qwen2.5-Omni-7B", "ModelScope"),
            (snapshot_download, "Qwen/Qwen2.5-Omni-7B", "Hugging Face")
        ]

        max_retries = 3
        for download_func, repo_id, source in download_sources:
            for retry in range(max_retries):
                logger.info(f"å°è¯•ä» {source} ä¸‹è½½æ¨¡å‹ (å°è¯• {retry + 1}/{max_retries})")
                try:
                    kwargs = {"cache_dir": self.model_path}
                    if download_func == snapshot_download:
                        kwargs["ignore_patterns"] = ["*.msgpack", "*.h5"]
                    download_func(repo_id, **kwargs)
                    logger.info(f"æˆåŠŸä» {source} ä¸‹è½½æ¨¡å‹")
                    return
                except Exception as e:
                    logger.warning(f"ä¸‹è½½å¤±è´¥: {e}")
                    if retry == max_retries - 1:
                        logger.error(f"ä» {source} ä¸‹è½½å¤±è´¥")
                    else:
                        time.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿

        raise RuntimeError("æ‰€æœ‰ä¸‹è½½æºå‡å¤±è´¥")

    # ... [ä¿ç•™åŸæœ‰çš„ tensor_to_pil å’Œ process æ–¹æ³•ä¸å˜]
    # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ä¿ç•™åŸæœ‰çš„ process æ–¹æ³•å®ç°ï¼Œåªæ˜¯æ²¡æœ‰å®Œæ•´è´´å‡ºä»¥èŠ‚çœç©ºé—´

NODE_CLASS_MAPPINGS = {
    "VideoUploader": VideoUploader,
    "QwenOmniCombined": QwenOmniCombined
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoUploader": "Video UploaderğŸ¼",
    "QwenOmniCombined": "Qwen Omni CombinedğŸ¼"
}
