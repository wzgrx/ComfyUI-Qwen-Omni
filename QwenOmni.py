import torch
import os
import io
from io import BytesIO
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor, BitsAndBytesConfig
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_omni_utils import process_mm_info
import numpy as np
import soundfile as sf
import torchaudio
device = "cuda"

def check_flash_attention():
    """Check if flash attention 2 is available"""
    try:
        from flash_attn import flash_attn_func
        return True
    except ImportError:
        return False

FLASH_ATTENTION_AVAILABLE = check_flash_attention()

def init_qwen_paths():
    """åŠ¨æ€åˆå§‹åŒ–æ¨¡åž‹è·¯å¾„"""
    qwen_base = Path(folder_paths.models_dir) / "Qwen"
    qwen_model_dir = qwen_base / "Qwen2.5-Omni-7B"
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    qwen_model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œè·¯å¾„åˆ°ç³»ç»Ÿ
    if not hasattr(folder_paths, "add_model_folder_path"):
        # å…¼å®¹æ—§ç‰ˆæœ¬æ‰‹åŠ¨æ³¨å†Œ
        if "Qwen" not in folder_paths.folder_names_and_paths:
            folder_paths.folder_names_and_paths["Qwen"] = ([str(qwen_model_dir)], {'.safetensors', '.bin'})
    else:
        folder_paths.add_model_folder_path("Qwen", str(qwen_model_dir))
    
    return str(qwen_model_dir)


class LoadQwenOmniModel:
    def __init__(self):
        self.model_path = init_qwen_paths()

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                
            }
        }

    RETURN_TYPES = ("QWENOMNI", "OMNIPROCESSOR")
    RETURN_NAMES = ("Qwen_Omni", "processor")
    FUNCTION = "load_model"
    CATEGORY = "ðŸ¼QwenOmni"
  
    def load_model(self):
        # æ·»åŠ Flash Attentionæ”¯æŒåˆ¤æ–­
        attn_implementation = "flash_attention_2" if FLASH_ATTENTION_AVAILABLE else None
        # æ·»åŠ é‡åŒ–é…ç½®
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,  # æ”¹ç”¨FP16åŠ é€Ÿè®¡ç®—
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,        # æ‰“å¼€orå…³é—­åŒé‡é‡åŒ–
            llm_int8_threshold=6.0,
        )

        


        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨æ›´é«˜æ•ˆçš„BF16æ ¼å¼
            quantization_config=quant_config,
            attn_implementation=attn_implementation,
            low_cpu_mem_usage=True,
            use_safetensors=True,  # å¯ç”¨æ›´å¿«çš„safetensorsæ ¼å¼
            offload_state_dict=True,  # ä¼˜åŒ–æ˜¾å­˜åˆ†é…
            # max_memory={0: "14GiB", "cpu": "64GiB"},  # ç²¾ç¡®æŽ§åˆ¶æ˜¾å­˜åˆ†é…
            enable_audio_output=True
        ).eval()

        
        # é¢„åŠ è½½æ¨¡åž‹åˆ°æ˜¾å­˜
        
        processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
        return model, processor


class QwenOmniParser:
    def __init__(self):
        self.model = None
        self.processor = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("QWENOMNI",),
                "processor": ("OMNIPROCESSOR",),
                "image": ("IMAGE",),
                "prompt": ("STRING", {
                    "default": "Describe this image in detail", 
                    "multiline": True
                }),
                "max_tokens": ("INT", {
                    "default": 128, 
                    "min": 32, 
                    "max": 512
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1
                }),
                "audio_mode": ([
                    "None (No Audio)", 
                    "Chelsie (Female)", 
                    "Ethan (Male)"
                ], {"default": "None (No Audio)"}),
            }
        }

    RETURN_TYPES = ("STRING", "AUDIO")
    RETURN_NAMES = ("text", "audio")
    FUNCTION = "analyze_image"
    CATEGORY = "ðŸ¼QwenOmni"

    def tensor_to_pil(self, image_tensor):
        """ä¼˜åŒ–å›¾åƒå¼ é‡è½¬æ¢"""
        # å¤„ç†æ‰¹æ¬¡ç»´åº¦ [B x H x W x C]
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
            
        # æ•°å€¼èŒƒå›´è½¬æ¢ [0-1] => [0-255]
        image_np = image_tensor.cpu().numpy()
        if image_np.max() <= 1.0:
            image_np = (image_np * 255).astype(np.uint8)
        else:
            image_np = image_np.astype(np.uint8)
            
        return Image.fromarray(image_np)
    

    def build_multimodal_inputs(self, pil_image, prompt, system_prompt):
        """æž„å»ºå¤šæ¨¡æ€è¾“å…¥ç»“æž„"""
        return [
            {
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pil_image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
    @torch.no_grad()
    def analyze_image(self, model, processor,image, prompt, max_tokens, temperature, audio_mode):
        # è½¬æ¢è¾“å…¥æ ¼å¼
        pil_image = self.tensor_to_pil(image)

        # å®šä¹‰åŒç³»ç»Ÿæç¤º â–¼â–¼â–¼
        DEFAULT_SYSTEM_PROMPT = "AI Assistant"
        OFFICIAL_AUDIO_PROMPT = "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."
        
        # æ ¹æ®éŸ³é¢‘å¼€å…³é€‰æ‹©æç¤ºè¯ â–¼â–¼â–¼
        system_prompt = OFFICIAL_AUDIO_PROMPT if enable_audio else DEFAULT_SYSTEM_PROMPT
        # æž„å»ºå¤šæ¨¡æ€å¯¹è¯
        conversation = self.build_multimodal_inputs(pil_image, prompt, system_prompt)
        
        # é¢„å¤„ç†å¤šæ¨¡æ€æ•°æ®
        text = processor.apply_chat_template(
            conversation,
            add_generation_prompt=True,
            tokenize=False
        )
        audios, images, videos = process_mm_info([conversation], use_audio_in_video=False)
        
        # è§£æžéŸ³é¢‘æ¨¡å¼å‚æ•° â–¼â–¼â–¼
        enable_audio = audio_mode != "None (No Audio)"
        # å½“ä¸”ä»…å½“ audio_mode ä¸æ˜¯ "None..." æ—¶å¯ç”¨éŸ³é¢‘
        
        voice_type = None
        if "Chelsie" in audio_mode:
            voice_type = "Chelsie"
        elif "Ethan" in audio_mode:
            voice_type = "Ethan"

        # å‡†å¤‡æ¨¡åž‹è¾“å…¥
        inputs = processor(
            text=text,
            images=images,
            audio=audios,
            videos=videos,
            return_tensors="pt",
            padding=True
        ).to(model.device)
        # # ç”Ÿæˆå‚æ•°é…ç½®
        generate_config = {
        "max_new_tokens": max_tokens,
        "do_sample": False,
        "temperature": temperature,
        "use_cache": True,
        "return_audio": enable_audio,  # è¿žæŽ¥è¾“å…¥å‚æ•°
        # "speaker": voice_type  # æŒ‡å®šå‘éŸ³äºº
        }

        # ä»…åœ¨å¯ç”¨éŸ³é¢‘æ—¶æ·»åŠ å‘éŸ³äººå‚æ•° â–¼â–¼â–¼
        if enable_audio and voice_type:
            generate_config["speaker"] = voice_type

        text_ids, audio = model.generate(**inputs,**generate_config)
        text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        return (text[0], audio)
        

class SaveQwenOmniAudio:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "filename": ("STRING", {"default": "output.wav"}),
                "samplerate": ("INT", {"default": 23000}),
            }
        }
    
    RETURN_TYPES = ()
    FUNCTION = "save_audio"
    OUTPUT_NODE = True
    CATEGORY = "ðŸ¼QwenOmni"    
    def save_audio(self, audio, filename, samplerate):
        # èŽ·å–ComfyUIçš„è¾“å‡ºç›®å½•
        output_dir = folder_paths.get_output_directory()
        
        # ç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„ï¼ˆé˜²æ­¢ç›®å½•æ³¨å…¥ï¼‰
        filename = os.path.basename(filename)
        
        # æž„å»ºå®Œæ•´ä¿å­˜è·¯å¾„
        full_path = os.path.join(output_dir, filename)
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        sf.write(
            full_path,
            audio.reshape(-1).detach().cpu().numpy(),
            samplerate=samplerate,
        )
        return ()



NODE_CLASS_MAPPINGS = {
    "LoadQwenOmniModel": LoadQwenOmniModel,
    "QwenOmniParser": QwenOmniParser,
    "SaveQwenOmniAudio": SaveQwenOmniAudio,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LoadQwenOmniModel": "Load QwenOmni ModelðŸ¼",
    "QwenOmniParser": "QwenOmni ParserðŸ¼",
    "SaveQwenOmniAudio": "Save QwenOmni AudioðŸ¼",
}